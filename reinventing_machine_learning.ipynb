{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinventing the wheel\n",
    "\n",
    "I wanted to try to derive the ideas of machine learning for myself. I do have some knowledge (so it's not completely independent of course), but I want to try to derive a lot of the ideas myself.\n",
    "\n",
    "The first sample problem is to write a machine learning algorithm to learn the xor function.\n",
    "\n",
    "So here's the xor logic table:\n",
    "\n",
    "| x0 | x1 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 0 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 1  | 1  | 0 |\n",
    "\n",
    "So we have a 2 input function that returns 1 value - something like this:\n",
    "\n",
    "<img src=\"images/xor_network1.png\"  style=\"width: 50%; height: 50%\" />\n",
    "\n",
    "My initial thought is that we can convolute through some linear model, and adjust coefficients to get the right answer.\n",
    "\n",
    "So the equation would look something like this:\n",
    "\n",
    "$W0*x0 + W1*x1 = y$\n",
    "\n",
    "You could put this into matrix form and add a bias coefficient, in which case it would look like this:\n",
    "\n",
    "$Ax +b = y$ where A is the vector containing the 2 weights, x is a free variable to hold the features - x0 and x1, and y is the vector of labels) - so something like this:\n",
    "\n",
    "<img src=\"images/xor_matrix_form1.png\"  style=\"width: 50%; height: 50%\" />\n",
    "\n",
    "Before trying to write an algorithm to optimize the values of W0 and W1, let's first make sure this model is powerful enough to work for the xor function. After thinking for a minute, I was able to determine that the values W0 = 1 and W2 = -1 would be good values, since if we plug those weights into the equation, $W0*x0 + W1*x1 = y$, we get $1*x0 + -1*x1$, and that should come out to the values of y (or at least, the value should be > 0.5 for (0, 1) and (1, 0), and < 0.5 for (0, 0) and (1, 1) ). And it turns out, that works:\n",
    "* for x0=0 and x1=1: `1*0 + -1*0 = 0`\n",
    "* for x0=0 and x1=1: `1*0 + -1*1 = -1`\n",
    "* for x0=0 and x1=1: `1*1 + -1*0 = 1`\n",
    "* for x0=0 and x1=1: `1*0 + -1*0 = 0`\n",
    "\n",
    "Shoot - when I first ran the numbers in my head, I was thinking the 2nd equation came out to 1, not -1. And actually, graphing it out, xor would look like this:\n",
    "\n",
    "<img src=\"images/xor_graph.png\"  style=\"width: 400px; height: 400px;\" />\n",
    "\n",
    "And as you can clearly see, that is not linearly separable. That is why, when I tried a linear classifier, it didn't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0]), array([0]), array([0]), array([0])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that I'm making 10x repetitions of the logic table.\n",
    "X = [[0, 0],\n",
    "     [0, 1],\n",
    "     [1, 0],\n",
    "     [1, 1]]*10\n",
    "y = [0, 1, 1, 0]*10\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y)\n",
    "\n",
    "[clf.predict([sample]) for sample in [(0, 0), (0, 1), (1, 0), (1, 1)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is wrong - presumably because a linear model simply isn't powerful enough, whereas something like RandomForest is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0]), array([1]), array([1]), array([0])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "[clf.predict([sample]) for sample in [(0, 0), (0, 1), (1, 0), (1, 1)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So RandomForest clearly works, but linear models do not. So we need to add some power to our model. How can we change it to be more powerful? Some ideas that come to mind:\n",
    "\n",
    "* Move from a linear to a polynomial model.\n",
    "* Add another layer to the network.\n",
    "\n",
    "Would these work? Is it possible that adding layers is similar in function to adding polynomials?\n",
    "\n",
    "Let's go with adding a layer...\n",
    "\n",
    "So the very simplest layer I could think of would be this:\n",
    "\n",
    "<img src=\"images/xor_network2.png\"  style=\"width: 50%; height: 50%;\" />\n",
    "(note: weights should be w0, w1, w2, and w3)\n",
    "\n",
    "But I know that typically, when I've seen pictures of neural networks, the inputs from one layer go to each of the nodes in the next layer - more like this:\n",
    "\n",
    "<img src=\"images/xor_network3.png\"  style=\"width: 50%; height: 50%;\" />\n",
    "(note: weights should be w0, w1, w2, and w3)\n",
    "\n",
    "I think the reason for this is that, in the prior network (without the cross in the middle), the 2 layers of weights are still only being added to the same input, which is really functionally the same as just having 1 layer. So I'm going to test out the network with the cross.\n",
    "\n",
    "So now we have 4 weights to play with. Let's see if I could imagine a solution where the weights are adjusted so that xor works... on second thought, that sounds like a good job for a program - let's do it that way.\n",
    "\n",
    "### Finding weights programmatically\n",
    "\n",
    "A few initial thoughts on writing a program to learn:\n",
    "* The inputs for the second layer are not the inputs, but the outputs of the previous layer.\n",
    "* We probably can't just keep 4 variables (one for each weight we are trying to tune), because we might optimize for the first sample, and then reoptimize for the second, and third, and so forth. One solution would be to generate a number of hypotheses for the weights, and keep them all in memory, and then find the one that minimizes error for all samples. Later, we can try to actually tweak the weights.\n",
    "\n",
    "So let's try to write this. First, some helper functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rand_weight(weight_min, weight_max):\n",
    "    \"\"\"Generates a uniformly random weight between weight_min and weight_max.\"\"\"\n",
    "    return random.random() * (weight_max - weight_min) + weight_min\n",
    "\n",
    "def gen_hypothesis(weight_min, weight_max, num_weights):\n",
    "    return tuple([rand_weight(weight_min, weight_max) for i in range(num_weights)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.938529171631436"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_weight(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.0073873187744695,\n",
       " -1.0547383843095992,\n",
       " 4.059153250556346,\n",
       " -4.575705844790714)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_hypothesis(-5, 5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node functionality\n",
    "\n",
    "Next, let's define the functionality of each node. I know there's the concept of an [Activation function](https://en.wikipedia.org/wiki/Activation_function) in common neural network terminology. I suppose there are several ways a given node could function:\n",
    "* It could always output some discrete value (like 1 and 0, or -1 and 1).\n",
    "* It could output a continuous value bounded within some range like a probability (e.g. 0 to 1).\n",
    "* It could output an unbounded continuous value.\n",
    "\n",
    "For starters, I'm going to just go with what seems the simplest - it's just going to output an unbounded continuous value taking by multiplying the input by the weight. So the equation to determine the output of a given node will be: $node(x) = w0 * x$.\n",
    "\n",
    "On second thought, though... now that we have a hidden layer, those nodes will have more than one input. So how should that be handled? It could be handled a few ways:\n",
    "* We could make each input have its own weight.\n",
    "* We could use the same weight for all of the inputs.\n",
    "\n",
    "I'm again going to choose the dumbest easiest approach and say each node only has a single weight (even though, if I recall correctly, neural networks typically have different weights for each input - but we'll get there eventually). And if there is more than 1 sample, we'll just add them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, weight):\n",
    "        self.weight = weight\n",
    "    def run(self, node_input):\n",
    "        if type(node_input) != list:\n",
    "            node_input = [node_input]\n",
    "        return sum([self.weight * input_val for input_val in node_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the node implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 50, 60]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Node(10)\n",
    "[n.run([1]), n.run([5]), n.run([1, 2, 3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Now let's try to write the network.\n",
    "\n",
    "Actually, just trying to write this, I immediately realize that we need to do some sort of convolution at the output node (to join its 2 inputs), so let's just add a 5th weight for that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net_1(weights, sample):\n",
    "    input_layer = [Node(weights[0]), Node(weights[1])]\n",
    "    hidden_layer = [Node(weights[3]), Node(weights[2])]\n",
    "    output_layer = Node(weights[4])\n",
    "    \n",
    "    input_layer_output = [input_layer[i].run(sample[i]) for i in range(len(input_layer))]\n",
    "    hidden_layer_output = [hidden_layer[i].run(input_layer_output) for i in range(len(hidden_layer))]\n",
    "    output_layer_output = output_layer.run(hidden_layer_output)\n",
    "    \n",
    "    return output_layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net_1([0, 0, 0, 0, 0], [1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net_1([1, 1, 1, 1, 1], [1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should double the above\n",
    "neural_net_1([1, 1, 1, 1, 2], [1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net_1([2, 2, 2, 2, 2], [10, 10, 10, 10, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work as expected. Let's now write an algorithm to generate a bunch of weight hypotheses, and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetPredictor:\n",
    "    def __init__(self, num_hypotheses=1000, weight_min=-5, weight_max=5):\n",
    "        # generate hypotheses randomly\n",
    "        self.hypotheses = [gen_hypothesis(weight_min, weight_max, 5) for i in range(num_hypotheses)]\n",
    "        self.hypothesis_to_error = {h: 0 for h in self.hypotheses}\n",
    "        self.trained_weights = [0]*5\n",
    "    \n",
    "    def add_sample(self, sample, expected_value):\n",
    "        for h in self.hypotheses:\n",
    "            network_value = neural_net_1(h, sample)\n",
    "            # print('sample = {}, network_value = {}'.format(sample, network_value))\n",
    "            err = (network_value - expected_value)**2\n",
    "            self.hypothesis_to_error[h] += err\n",
    "    \n",
    "    def train(self, samples, labels):\n",
    "        for index in range(len(samples)):\n",
    "            self.add_sample(samples[index], labels[index])\n",
    "        \n",
    "        # Find hypothesis with lowest error\n",
    "        best_hypothesis = self.hypotheses[0]\n",
    "        lowest_err = 10000000\n",
    "        for hyp_index in range(len(self.hypotheses)):\n",
    "            hyp = self.hypotheses[hyp_index]\n",
    "            err = self.hypothesis_to_error[hyp]\n",
    "            if err < lowest_err:\n",
    "                lowest_err = err\n",
    "                best_hypothesis = hyp\n",
    "        \n",
    "        self.trained_weights = best_hypothesis\n",
    "        print('Best hypothesis weights: {}'.format(best_hypothesis))\n",
    " \n",
    "    def predict(self, sample):\n",
    "        return neural_net_1(self.trained_weights, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hypothesis weights: (-3.448188486114756, -2.906375898177732, 1.2268473056308373, -1.2651189785360972, 2.685541103398994)\n",
      "Predictions: [0.0, 0.2987177527559055, 0.3544053321859728, 0.6531230849418748]\n"
     ]
    }
   ],
   "source": [
    "# Note that I'm making 10x repetitions of the logic table.\n",
    "X = [[0, 0],\n",
    "     [0, 1],\n",
    "     [1, 0],\n",
    "     [1, 1]]*10\n",
    "y = [0, 1, 1, 0]*10\n",
    "\n",
    "xor_predictor = NeuralNetPredictor()\n",
    "xor_predictor.train(X, y)\n",
    "print('Predictions: {}'.format([xor_predictor.predict(x) for x in [(0, 0), (0, 1), (1, 0), (1, 1)]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hypothesis weights: (-3.203594529830033, -4.496694842845448, 2.4246931548759516, -2.4409435203483087, 4.405656218287904)\n",
      "Predictions: [0.0, 0.32193422877027444, 0.2293566208733253, 0.5512908496435927]\n"
     ]
    }
   ],
   "source": [
    "xor_predictor = NeuralNetPredictor()\n",
    "xor_predictor.train(X, y)\n",
    "print('Predictions: {}'.format([xor_predictor.predict(x) for x in [(0, 0), (0, 1), (1, 0), (1, 1)]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hypothesis weights: (-4.93163851348113, -4.501216296913628, -4.806145566496756, 4.912783016586257, -0.5993876984644997)\n",
      "Predictions: [0.0, 0.2877050332703366, 0.31521640574598564, 0.602921439016324]\n"
     ]
    }
   ],
   "source": [
    "xor_predictor = NeuralNetPredictor()\n",
    "xor_predictor.train(X, y)\n",
    "print('Predictions: {}'.format([xor_predictor.predict(x) for x in [(0, 0), (0, 1), (1, 0), (1, 1)]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It doesn't work!\n",
    "\n",
    "Okay, so this is clearly not working. At this point, we're not using converting into classes based on the output value, but still, I'd expect the output values to be higher at (0, 1) and (1, 0). But instead, it seems like they pretty much correspond to the input values (1, 1) is almost always the highest, (0, 0) is always 0, and (0, 1) and (1, 0) are almost always about half of (1, 1). Interestingly, the 4 classes summed together is very near 1 - which is kind of surprising considering I'm not explicitly doing any probability normalizations.\n",
    "\n",
    "So why is this not working? My guess is that, since the \"expected value\" used to calculate the error with is always either 0 or 1, it's being almost entirely factored out by large network output values. So to fix this, we'd probably want the network values to be something in the 0 to 1 range (to match the classes).\n",
    "\n",
    "Well, we'll try that another day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
